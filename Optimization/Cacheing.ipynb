{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets talk about CHaching \n",
    "What is cache?\n",
    "\n",
    "\n",
    "\n",
    "storing the data frame or a set of logics in memeroy so that we can reuse them down the oder with our wasting our resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/19 16:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\",\"10g\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"268435456\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc= spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcustomers.parquet\u001b[m\u001b[m    \u001b[34mtransactions.parquet\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data/data_skew/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city       |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|10   |7  |Entertainment|10.42 |boston     |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|3    |27 |Motor/Travel |44.34 |portland   |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|4    |11 |Entertainment|3.18  |chicago    |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|2    |22 |Groceries    |268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|10   |16 |Entertainment|2.66  |chicago    |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Customers: 5000\n",
      "Transactions: 39790092\n"
     ]
    }
   ],
   "source": [
    "# Load both parquet datasets\n",
    "df_customers = spark.read.parquet(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data/data_skew/customers.parquet\")\n",
    "df_transactions = spark.read.parquet(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data/data_skew/transactions.parquet\")\n",
    "\n",
    "# Show first few records\n",
    "df_customers.show(5, truncate=False)\n",
    "df_transactions.show(5, truncate=False)\n",
    "\n",
    "# Print schemas\n",
    "df_customers.printSchema()\n",
    "df_transactions.printSchema()\n",
    "\n",
    "# Count records\n",
    "print(\"Customers:\", df_customers.count())\n",
    "print(\"Transactions:\", df_transactions.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+---------+-----+------+--------------+\n",
      "|cust_id   |name          |age|gender|birthday |zip  |city  |customer_group|\n",
      "+----------+--------------+---+------+---------+-----+------+--------------+\n",
      "|C007YEYTX9|Aaron Abbott  |34 |Female|7/13/1991|97823|boston|mid           |\n",
      "|C08XAQUY73|Aaron Lambert |54 |Female|11/5/1966|75218|boston|old           |\n",
      "|C094P1VXF9|Aaron Lindsey |24 |Male  |9/21/1990|29399|boston|young         |\n",
      "|C097SHE1EF|Aaron Lopez   |22 |Female|4/18/2001|82129|boston|young         |\n",
      "|C0DTC6436T|Aaron Schwartz|52 |Female|7/9/1962 |57192|boston|old           |\n",
      "+----------+--------------+---+------+---------+-----+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_base = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"city\") == \"boston\")\n",
    "    .withColumn(\n",
    "        \"customer_group\", \n",
    "        F.when(\n",
    "            F.col(\"age\").between(20, 30), \n",
    "            F.lit(\"young\") \n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"age\").between(31, 50), \n",
    "            F.lit(\"mid\") \n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"age\") > 51, \n",
    "            F.lit(\"old\") \n",
    "        )\n",
    "        .otherwise(F.lit(\"kid\"))\n",
    "     )\n",
    "    .select(\"cust_id\", \"name\", \"age\", \"gender\", \"birthday\", \"zip\", \"city\", \"customer_group\")\n",
    ")\n",
    "#as you can see that the data frame named df_base is going many tarnsformations\n",
    "#we are doing filtering adding new columns \n",
    "# df_base.cache()  for now lets now cache it\n",
    "df_base.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, CASE WHEN ((cast(age#2 as bigint) >= 20) AND (cast(age#2 as bigint) <= 30)) THEN young WHEN ((cast(age#2 as bigint) >= 31) AND (cast(age#2 as bigint) <= 50)) THEN mid WHEN (cast(age#2 as bigint) > 51) THEN old ELSE kid END AS customer_group#124]\n",
      "+- *(1) Filter (isnotnull(city#6) AND (city#6 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] Batched: true, DataFilters: [isnotnull(city#6), (city#6 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_base.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, CASE WHEN ((cast(age#2 as bigint) >= 20) AND (cast(age#2 as bigint) <= 30)) THEN young WHEN ((cast(age#2 as bigint) >= 31) AND (cast(age#2 as bigint) <= 50)) THEN mid WHEN (cast(age#2 as bigint) > 51) THEN old ELSE kid END AS customer_group#124, test_column_1 AS test_column_1#183, split(birthday#4, /, -1)[2] AS birth_year#184]\n",
      "+- *(1) Filter (isnotnull(city#6) AND (city#6 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] Batched: true, DataFilters: [isnotnull(city#6), (city#6 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n",
      "\n",
      "+----------+--------------+---+------+---------+-----+------+--------------+-------------+----------+\n",
      "|cust_id   |name          |age|gender|birthday |zip  |city  |customer_group|test_column_1|birth_year|\n",
      "+----------+--------------+---+------+---------+-----+------+--------------+-------------+----------+\n",
      "|C007YEYTX9|Aaron Abbott  |34 |Female|7/13/1991|97823|boston|mid           |test_column_1|1991      |\n",
      "|C08XAQUY73|Aaron Lambert |54 |Female|11/5/1966|75218|boston|old           |test_column_1|1966      |\n",
      "|C094P1VXF9|Aaron Lindsey |24 |Male  |9/21/1990|29399|boston|young         |test_column_1|1990      |\n",
      "|C097SHE1EF|Aaron Lopez   |22 |Female|4/18/2001|82129|boston|young         |test_column_1|2001      |\n",
      "|C0DTC6436T|Aaron Schwartz|52 |Female|7/9/1962 |57192|boston|old           |test_column_1|1962      |\n",
      "+----------+--------------+---+------+---------+-----+------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#lets create a dataframe on top of df_base and see what is happening\n",
    "df1 = (\n",
    "    df_base\n",
    "    .withColumn(\"test_column_1\", F.lit(\"test_column_1\"))\n",
    "    .withColumn(\"birth_year\", F.split(\"birthday\", \"/\").getItem(2))\n",
    ")\n",
    "df1.explain()\n",
    "df1.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see you df_base transformations are going again from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cust_id: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string, customer_group: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =(\n",
    "    df_base    .withColumn(\"test_column_2\", F.lit(\"test_column_2\"))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, customer_group#124, test_column_2 AS test_column_2#256]\n",
      "+- InMemoryTableScan [age#2, birthday#4, city#6, cust_id#0, customer_group#124, gender#3, name#1, zip#5]\n",
      "      +- InMemoryRelation [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, customer_group#124], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Project [cust_id#0, name#1, age#2, gender#3, birthday#4, zip#5, city#6, CASE WHEN ((cast(age#2 as bigint) >= 20) AND (cast(age#2 as bigint) <= 30)) THEN young WHEN ((cast(age#2 as bigint) >= 31) AND (cast(age#2 as bigint) <= 50)) THEN mid WHEN (cast(age#2 as bigint) > 51) THEN old ELSE kid END AS customer_group#124]\n",
      "               +- *(1) Filter (isnotnull(city#6) AND (city#6 = boston))\n",
      "                  +- *(1) ColumnarToRow\n",
      "                     +- FileScan parquet [cust_id#0,name#1,age#2,gender#3,birthday#4,zip#5,city#6] Batched: true, DataFilters: [isnotnull(city#6), (city#6 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/bhushanchowdary/Documents/GitHub/pyspark/Optimization/data..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  ** INMEMORY TABLE SCAN ** is happening in here\n",
    "* what that means is we are direclty building our logic for df2 on top of df_base with rescaning and operating the whole thing again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CACHE VS PERSIST\n",
    "both are Same the only differnce is storage level you can metion that for persist \n",
    "\n",
    "    why that matters \n",
    "    the default is memomry and disk deserialized deserilization make it fast access so cpu efficent but not so memery efficient\n",
    "    disk only cpu efficeint and disk efficent but slow access of the data and the data is serilized when its is stored in the disk\n",
    "    memoruy only as name suggests it is memory only but fast access less memmory effesicent\n",
    "\n",
    "### When to use what?\n",
    "```\n",
    "Storage Level    Space used  CPU time  In memory  On-disk  Serialized\n",
    "---------------------------------------------------------------------\n",
    "MEMORY_ONLY          High        Low       Y          N        N         \n",
    "MEMORY_ONLY_SER      Low         High      Y          N        Y     \n",
    "MEMORY_AND_DISK      High        Medium    Some       Some     Some  \n",
    "MEMORY_AND_DISK_SER  Low         High      Some       Some     Y     \n",
    "DISK_ONLY            Low         High      N          Y        Y     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cust_id: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string, customer_group: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
