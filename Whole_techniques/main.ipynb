{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/20 18:08:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/20 18:08:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"MyApp\")   \n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> partition pruning<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkeing AQE STATUS\n",
    "spark.conf.get(\"spark.sql.adaptive.enabled\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")         # if the CSV has headers\n",
    "    .option(\"inferSchema\", \"true\")    # enable schema inference\n",
    "    .csv(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Whole_techniques/BigMart Sales.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+---------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|Item_Type            |Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|Outlet_Type      |Item_Outlet_Sales|\n",
      "+---------------+-----------+----------------+---------------+---------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|FDA15          |9.3        |Low Fat         |0.016047301    |Dairy                |249.8092|OUT049           |1999                     |Medium     |Tier 1              |Supermarket Type1|3735.138         |\n",
      "|DRC01          |5.92       |Regular         |0.019278216    |Soft Drinks          |48.2692 |OUT018           |2009                     |Medium     |Tier 3              |Supermarket Type2|443.4228         |\n",
      "|FDN15          |17.5       |Low Fat         |0.016760075    |Meat                 |141.618 |OUT049           |1999                     |Medium     |Tier 1              |Supermarket Type1|2097.27          |\n",
      "|FDX07          |19.2       |Regular         |0.0            |Fruits and Vegetables|182.095 |OUT010           |1998                     |NULL       |Tier 3              |Grocery Store    |732.38           |\n",
      "|NCD19          |8.93       |Low Fat         |0.0            |Household            |53.8614 |OUT013           |1987                     |High       |Tier 3              |Supermarket Type1|994.7052         |\n",
      "+---------------+-----------+----------------+---------------+---------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\",131072)#partitiong the data based on the size of the data useually the recommended size for spark is 128 mb spark recommended (100-200) but maximum block sizes \n",
    "# between  128 so deafult size\n",
    "#to make the partiions configurations based on the size one must change this or set the conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Repartitions <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(partition_number)|\n",
      "+-----------------------+\n",
      "|                   8523|\n",
      "+-----------------------+\n",
      "\n",
      "+----------------+-----+\n",
      "|partition_number|count|\n",
      "+----------------+-----+\n",
      "|               1|  854|\n",
      "|               6|  851|\n",
      "|               3|  852|\n",
      "|               5|  850|\n",
      "|               9|  853|\n",
      "|               4|  853|\n",
      "|               8|  852|\n",
      "|               7|  851|\n",
      "|               2|  854|\n",
      "|               0|  853|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "df_with_part = df.withColumn(\"partition_number\", F.spark_partition_id())#this is too add the partition id for the row `F.spark_partition_id()`\n",
    "\n",
    "# total count of non-null partition_number values\n",
    "df_with_part.select(F.count(\"partition_number\")).show()\n",
    "\n",
    "# OR distribution of rows across partitions\n",
    "df_with_part.groupBy(\"partition_number\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data writing for our partitions<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/20 18:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/20 18:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/20 18:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/20 18:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/20 18:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Whole_techniques/partitions/partition_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.parquet(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Whole_techniques/partitions/partition_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+--------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|           Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|      Outlet_Type|Item_Outlet_Sales|\n",
      "+---------------+-----------+----------------+---------------+--------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|          FDB56|       8.75|         Regular|    0.074931201|Fruits and Vegeta...|187.6556|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|          938.778|\n",
      "|          NCK17|       11.0|         Low Fat|    0.037971697|  Health and Hygiene|  38.948|           OUT045|                     2002|       NULL|              Tier 2|Supermarket Type1|          519.324|\n",
      "|          FDO34|       NULL|         Low Fat|    0.029793955|         Snack Foods|167.2816|           OUT027|                     1985|     Medium|              Tier 3|Supermarket Type3|        5704.5744|\n",
      "|          FDP09|      19.75|         Low Fat|    0.034027909|         Snack Foods|212.0902|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|         3185.853|\n",
      "|          DRD60|       NULL|         Low Fat|    0.065188619|         Soft Drinks|181.1634|           OUT019|                     1985|      Small|              Tier 1|    Grocery Store|         181.7634|\n",
      "|          FDL25|       6.92|         Regular|    0.131128467|           Breakfast| 93.1804|           OUT049|                     1999|     Medium|              Tier 1|Supermarket Type1|        1561.9668|\n",
      "|          FDA02|       14.0|         Regular|    0.029843736|               Dairy|143.9786|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|         866.8716|\n",
      "|          NCX29|       NULL|         Low Fat|    0.156094569|  Health and Hygiene|144.3102|           OUT019|                     1985|      Small|              Tier 1|    Grocery Store|         145.8102|\n",
      "|          FDX07|       19.2|         Regular|            0.0|Fruits and Vegeta...| 182.095|           OUT010|                     1998|       NULL|              Tier 3|    Grocery Store|           732.38|\n",
      "|          FDA03|       18.5|         Regular|    0.045463773|               Dairy|144.1102|           OUT046|                     1997|      Small|              Tier 1|Supermarket Type1|         2187.153|\n",
      "|          NCI17|      8.645|         Low Fat|    0.143713508|  Health and Hygiene|  96.641|           OUT045|                     2002|       NULL|              Tier 2|Supermarket Type1|          868.869|\n",
      "|          FDU37|        9.5|         Regular|    0.104421237|              Canned|  80.196|           OUT013|                     1987|       High|              Tier 3|Supermarket Type1|         2157.192|\n",
      "|          FDC17|      12.15|         Low Fat|    0.015523707|        Frozen Foods|212.0928|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|        2735.1064|\n",
      "|          FDU37|        9.5|         Regular|    0.104933928|              Canned|  78.796|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|          1198.44|\n",
      "|          FDC32|      18.35|         Low Fat|    0.099309997|Fruits and Vegeta...| 91.4462|           OUT045|                     2002|       NULL|              Tier 2|Supermarket Type1|        1665.8316|\n",
      "|          FDB21|       NULL|         Low Fat|            0.0|Fruits and Vegeta...|242.9854|           OUT027|                     1985|     Medium|              Tier 3|Supermarket Type3|        6767.1912|\n",
      "|          NCN18|      8.895|         Low Fat|    0.125222705|           Household|113.3544|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|        1006.6896|\n",
      "|          FDM21|       NULL|         Low Fat|    0.064052392|         Snack Foods|256.1646|           OUT027|                     1985|     Medium|              Tier 3|Supermarket Type3|        7472.2734|\n",
      "|          FDH45|       15.1|             reg|    0.105646853|Fruits and Vegeta...| 42.8796|           OUT035|                     2004|      Small|              Tier 2|Supermarket Type1|         949.4308|\n",
      "|          FDS15|      9.195|         Regular|    0.078502143|                Meat|108.7596|           OUT017|                     2007|       NULL|              Tier 2|Supermarket Type1|        2588.6304|\n",
      "+---------------+-----------+----------------+---------------+--------------------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").partitionBy(\"Outlet_Location_Type\").mode(\"append\").save(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Whole_techniques/partitions/partition_opt_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.parquet(\"/Users/bhushanchowdary/Documents/GitHub/pyspark/Whole_techniques/partitions/partition_opt_01\")\n",
    "df_new = df_new.filter(col(\"Outlet_Location_Type\")==\"Tier 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is called static pruning we are applying pruning at the first step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join optimizations \n",
    "\n",
    "Brodcast joins 10mb to 8gb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tran = spark.createDataFrame(\n",
    "    (),\n",
    "    (),\n",
    "    (),\n",
    "    (),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
